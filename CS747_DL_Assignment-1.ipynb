{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_CIFAR10_data\n",
    "from scipy.spatial import distance\n",
    "from models import Perceptron, SVM, Softmax\n",
    "from kaggle_submission import output_submission_csv\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we determine the number of images for each split and load the images.\n",
    "<br /> \n",
    "TRAIN_IMAGES + VAL_IMAGES = (0, 50000]\n",
    ", TEST_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 40000\n",
    "VAL_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES)\n",
    "X_train_CIFAR, y_train_CIFAR = data['X_train'], data['y_train']\n",
    "X_val_CIFAR, y_val_CIFAR = data['X_val'], data['y_val']\n",
    "X_test_CIFAR, y_test_CIFAR = data['X_test'], data['y_test']\n",
    "n_class_CIFAR = len(np.unique(y_test_CIFAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CIFAR = np.reshape(X_train_CIFAR, (X_train_CIFAR.shape[0], -1))\n",
    "X_val_CIFAR = np.reshape(X_val_CIFAR, (X_val_CIFAR.shape[0], -1))\n",
    "X_test_CIFAR = np.reshape(X_test_CIFAR, (X_test_CIFAR.shape[0], -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes how well your model performs using accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test == pred) / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron has 2 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, but you should experiment with different values. We recommend changing the learning rate by factors of 10 and observing how the performance of the classifier changes. You should also try adding a **decay** which slowly reduces the learning rate over each epoch.\n",
    "- **Number of Epochs** - An epoch is a complete iterative pass over all of the data in the dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according to the perceptron update rule for each sample in the training set. You should try different values for the number of training epochs and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the Perceptron classifier in the **models/perceptron.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Perceptron classifier class \n",
    "- The train function of the Perceptron class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 1. 1. ... 1. 1. 1.]]\n",
      "(11, 3072)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.50\n",
    "n_epochs = 10\n",
    "\n",
    "percept_CIFAR = Perceptron(n_class_CIFAR, lr, n_epochs)\n",
    "percept_CIFAR.train(X_train_CIFAR, y_train_CIFAR)\n",
    "\n",
    "# from sklearn.linear_model import Perceptron\n",
    "# percept_CIFAR = Perceptron(alpha = lr, max_iter = n_epochs)\n",
    "# percept_CIFAR.fit(X_train_CIFAR, y_train_CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 35.697500\n"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_CIFAR.predict(X_train_CIFAR)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 28.710000\n"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_CIFAR.predict(X_val_CIFAR)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 28.830000\n"
     ]
    }
   ],
   "source": [
    "pred_percept = percept_CIFAR.predict(X_test_CIFAR)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_CIFAR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m temp_n_epochs \u001b[39min\u001b[39;00m n_epoch_list:\n\u001b[1;32m     10\u001b[0m     temp_model \u001b[39m=\u001b[39m Perceptron(n_class_CIFAR, temp_lr, temp_n_epochs)\n\u001b[0;32m---> 11\u001b[0m     temp_model\u001b[39m.\u001b[39;49mtrain(X_train_CIFAR, y_train_CIFAR)\n\u001b[1;32m     13\u001b[0m     temp_pred \u001b[39m=\u001b[39m temp_model\u001b[39m.\u001b[39mpredict(X_val_CIFAR)\n\u001b[1;32m     15\u001b[0m     prec_hyper_list\u001b[39m.\u001b[39mappend((temp_lr, temp_n_epochs))\n",
      "File \u001b[0;32m/workspaces/linear_classifers_747/models/perceptron.py:57\u001b[0m, in \u001b[0;36mPerceptron.train\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m epoch_num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m index, data_row \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(X_train):\n\u001b[1;32m     55\u001b[0m         \u001b[39m# data_row = (1, D)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         \u001b[39m# weights = (Num_Classes, D)\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m         y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(np\u001b[39m.\u001b[39;49mdot(data_row, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mw\u001b[39m.\u001b[39;49mT))\n\u001b[1;32m     59\u001b[0m         \u001b[39m# y_pred = 1\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         y_correct \u001b[39m=\u001b[39m y_train[index]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# HyperParameter tuning for Perceptron\n",
    "\n",
    "lr_list = [0.00001, 0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.50, 0.75]\n",
    "n_epoch_list = [5, 10, 25, 50, 75, 100, 250, 500, 750, 1000]\n",
    "\n",
    "prec_hyper_list = []\n",
    "acc_list = []\n",
    "for temp_lr in lr_list: \n",
    "    for temp_n_epochs in n_epoch_list:\n",
    "        temp_model = Perceptron(n_class_CIFAR, temp_lr, temp_n_epochs)\n",
    "        temp_model.train(X_train_CIFAR, y_train_CIFAR)\n",
    "\n",
    "        temp_pred = temp_model.predict(X_val_CIFAR)\n",
    "\n",
    "        # List of parameters with it's associated accuracy. \n",
    "        prec_hyper_list.append((temp_lr, temp_n_epochs))\n",
    "        acc_list.append(get_acc(temp_pred, y_val_CIFAR))\n",
    "\n",
    "\n",
    "best_perc_index = np.argmax(np.array(acc_list))\n",
    "print(f\"best_accuracy - {acc_list[best_perc_index]}, hyperparameters - {prec_hyper_list[best_perc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/perceptron_submission_CIFAR.csv', percept_CIFAR.predict(X_test_CIFAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Epochs** - similar to as defined above in Perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the SVM using SGD in the **models/svm.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the SVM classifier class \n",
    "- The train function of the SVM class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SGDClassifier\n\u001b[1;32m     10\u001b[0m svm_CIFAR \u001b[39m=\u001b[39m SGDClassifier(alpha \u001b[39m=\u001b[39m lr, learning_rate\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, eta0 \u001b[39m=\u001b[39m reg_const)\n\u001b[0;32m---> 11\u001b[0m svm_CIFAR\u001b[39m.\u001b[39;49mfit(X_train_CIFAR, y_train_CIFAR)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:894\u001b[0m, in \u001b[0;36mBaseSGDClassifier.fit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m    892\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_more_validate_params()\n\u001b[0;32m--> 894\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    895\u001b[0m     X,\n\u001b[1;32m    896\u001b[0m     y,\n\u001b[1;32m    897\u001b[0m     alpha\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malpha,\n\u001b[1;32m    898\u001b[0m     C\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    899\u001b[0m     loss\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    900\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate,\n\u001b[1;32m    901\u001b[0m     coef_init\u001b[39m=\u001b[39;49mcoef_init,\n\u001b[1;32m    902\u001b[0m     intercept_init\u001b[39m=\u001b[39;49mintercept_init,\n\u001b[1;32m    903\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    904\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:683\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39m# Clear iteration count for multiple call to fit.\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt_ \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m--> 683\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_partial_fit(\n\u001b[1;32m    684\u001b[0m     X,\n\u001b[1;32m    685\u001b[0m     y,\n\u001b[1;32m    686\u001b[0m     alpha,\n\u001b[1;32m    687\u001b[0m     C,\n\u001b[1;32m    688\u001b[0m     loss,\n\u001b[1;32m    689\u001b[0m     learning_rate,\n\u001b[1;32m    690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    691\u001b[0m     classes,\n\u001b[1;32m    692\u001b[0m     sample_weight,\n\u001b[1;32m    693\u001b[0m     coef_init,\n\u001b[1;32m    694\u001b[0m     intercept_init,\n\u001b[1;32m    695\u001b[0m )\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    699\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtol \u001b[39m>\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[1;32m    700\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_iter\n\u001b[1;32m    701\u001b[0m ):\n\u001b[1;32m    702\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    703\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMaximum number of iteration reached before \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mconvergence. Consider increasing max_iter to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    705\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mimprove the fit.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    706\u001b[0m         ConvergenceWarning,\n\u001b[1;32m    707\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:617\u001b[0m, in \u001b[0;36mBaseSGDClassifier._partial_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39m# delegate to concrete training procedure\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m n_classes \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m--> 617\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_multiclass(\n\u001b[1;32m    618\u001b[0m         X,\n\u001b[1;32m    619\u001b[0m         y,\n\u001b[1;32m    620\u001b[0m         alpha\u001b[39m=\u001b[39;49malpha,\n\u001b[1;32m    621\u001b[0m         C\u001b[39m=\u001b[39;49mC,\n\u001b[1;32m    622\u001b[0m         learning_rate\u001b[39m=\u001b[39;49mlearning_rate,\n\u001b[1;32m    623\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    624\u001b[0m         max_iter\u001b[39m=\u001b[39;49mmax_iter,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[1;32m    626\u001b[0m \u001b[39melif\u001b[39;00m n_classes \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_binary(\n\u001b[1;32m    628\u001b[0m         X,\n\u001b[1;32m    629\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m         max_iter\u001b[39m=\u001b[39mmax_iter,\n\u001b[1;32m    635\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:760\u001b[0m, in \u001b[0;36mBaseSGDClassifier._fit_multiclass\u001b[0;34m(self, X, y, alpha, C, learning_rate, sample_weight, max_iter)\u001b[0m\n\u001b[1;32m    758\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[1;32m    759\u001b[0m seeds \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_))\n\u001b[0;32m--> 760\u001b[0m result \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    761\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    762\u001b[0m )(\n\u001b[1;32m    763\u001b[0m     delayed(fit_binary)(\n\u001b[1;32m    764\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    765\u001b[0m         i,\n\u001b[1;32m    766\u001b[0m         X,\n\u001b[1;32m    767\u001b[0m         y,\n\u001b[1;32m    768\u001b[0m         alpha,\n\u001b[1;32m    769\u001b[0m         C,\n\u001b[1;32m    770\u001b[0m         learning_rate,\n\u001b[1;32m    771\u001b[0m         max_iter,\n\u001b[1;32m    772\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_expanded_class_weight[i],\n\u001b[1;32m    773\u001b[0m         \u001b[39m1.0\u001b[39;49m,\n\u001b[1;32m    774\u001b[0m         sample_weight,\n\u001b[1;32m    775\u001b[0m         validation_mask\u001b[39m=\u001b[39;49mvalidation_mask,\n\u001b[1;32m    776\u001b[0m         random_state\u001b[39m=\u001b[39;49mseed,\n\u001b[1;32m    777\u001b[0m     )\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, seed \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(seeds)\n\u001b[1;32m    779\u001b[0m )\n\u001b[1;32m    781\u001b[0m \u001b[39m# take the maximum of n_iter_ over every binary fit\u001b[39;00m\n\u001b[1;32m    782\u001b[0m n_iter_ \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/joblib/parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/sklearn/linear_model/_stochastic_gradient.py:446\u001b[0m, in \u001b[0;36mfit_binary\u001b[0;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[1;32m    442\u001b[0m seed \u001b[39m=\u001b[39m random_state\u001b[39m.\u001b[39mrandint(MAX_INT)\n\u001b[1;32m    444\u001b[0m tol \u001b[39m=\u001b[39m est\u001b[39m.\u001b[39mtol \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39mtol \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf\n\u001b[0;32m--> 446\u001b[0m coef, intercept, average_coef, average_intercept, n_iter_ \u001b[39m=\u001b[39m _plain_sgd(\n\u001b[1;32m    447\u001b[0m     coef,\n\u001b[1;32m    448\u001b[0m     intercept,\n\u001b[1;32m    449\u001b[0m     average_coef,\n\u001b[1;32m    450\u001b[0m     average_intercept,\n\u001b[1;32m    451\u001b[0m     est\u001b[39m.\u001b[39;49mloss_function_,\n\u001b[1;32m    452\u001b[0m     penalty_type,\n\u001b[1;32m    453\u001b[0m     alpha,\n\u001b[1;32m    454\u001b[0m     C,\n\u001b[1;32m    455\u001b[0m     est\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[1;32m    456\u001b[0m     dataset,\n\u001b[1;32m    457\u001b[0m     validation_mask,\n\u001b[1;32m    458\u001b[0m     est\u001b[39m.\u001b[39;49mearly_stopping,\n\u001b[1;32m    459\u001b[0m     validation_score_cb,\n\u001b[1;32m    460\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mn_iter_no_change),\n\u001b[1;32m    461\u001b[0m     max_iter,\n\u001b[1;32m    462\u001b[0m     tol,\n\u001b[1;32m    463\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mfit_intercept),\n\u001b[1;32m    464\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mverbose),\n\u001b[1;32m    465\u001b[0m     \u001b[39mint\u001b[39;49m(est\u001b[39m.\u001b[39;49mshuffle),\n\u001b[1;32m    466\u001b[0m     seed,\n\u001b[1;32m    467\u001b[0m     pos_weight,\n\u001b[1;32m    468\u001b[0m     neg_weight,\n\u001b[1;32m    469\u001b[0m     learning_rate_type,\n\u001b[1;32m    470\u001b[0m     est\u001b[39m.\u001b[39;49meta0,\n\u001b[1;32m    471\u001b[0m     est\u001b[39m.\u001b[39;49mpower_t,\n\u001b[1;32m    472\u001b[0m     \u001b[39m0\u001b[39;49m,\n\u001b[1;32m    473\u001b[0m     est\u001b[39m.\u001b[39;49mt_,\n\u001b[1;32m    474\u001b[0m     intercept_decay,\n\u001b[1;32m    475\u001b[0m     est\u001b[39m.\u001b[39;49maverage,\n\u001b[1;32m    476\u001b[0m )\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m est\u001b[39m.\u001b[39maverage:\n\u001b[1;32m    479\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(est\u001b[39m.\u001b[39mclasses_) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "# svm_CIFAR = SVM(n_class_CIFAR, lr, n_epochs, reg_const)\n",
    "# svm_CIFAR.train(X_train_CIFAR, y_train_CIFAR)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_CIFAR = SGDClassifier(alpha = lr, learning_rate=\"constant\", eta0 = reg_const)\n",
    "svm_CIFAR.fit(X_train_CIFAR, y_train_CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is given by: 9.025000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_CIFAR.predict(X_train_CIFAR)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation accuracy is given by: 9.110000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_CIFAR.predict(X_val_CIFAR)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy is given by: 8.690000\n"
     ]
    }
   ],
   "source": [
    "pred_svm = svm_CIFAR.predict(X_test_CIFAR)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/svm_submission_CIFAR.csv', svm_CIFAR.predict(X_test_CIFAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
    "\n",
    "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement a softmax classifier using SGD in the **models/softmax.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Softmax classifier class \n",
    "- The train function of the Softmax class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "n_epochs = 10\n",
    "reg_const = 0.05\n",
    "\n",
    "softmax_CIFAR = Softmax(n_class_CIFAR, lr, n_epochs, reg_const)\n",
    "softmax_CIFAR.train(X_train_CIFAR, y_train_CIFAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_CIFAR.predict(X_train_CIFAR)\n",
    "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_CIFAR.predict(X_val_CIFAR)\n",
    "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_softmax = softmax_CIFAR.predict(X_test_CIFAR)\n",
    "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_CIFAR)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_submission_csv('kaggle/softmax_submission_CIFAR.csv', softmax_CIFAR.predict(X_test_CIFAR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
